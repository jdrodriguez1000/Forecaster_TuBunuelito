# Blueprint de IngenierÃ­a: Fase 01 - Data Extraction & Synchronization (Loader)

Este documento no solo registra el resultado, sino que sirve como la **GuÃ­a Maestra de IngenierÃ­a** para replicar la ingesta incremental de datos en cualquier proyecto de series de tiempo bajo el estÃ¡ndar de Sabbia Solutions.

---

## 1. ðŸŽ¯ Objetivos EstratÃ©gicos (The North Star)
*   **Eficiencia de Ancho de Banda:** Cargar solo lo necesario (delta incremental).
*   **Inmutabilidad del Dato Crudo:** Preservar la fuente de verdad en `/data/01_raw/`.
*   **CertificaciÃ³n AutomÃ¡tica:** Ninguna tabla entra al pipeline sin pasar el "Data Contract".

---

## 2. ðŸ—ï¸ Arquitectura del Motor de Ingesta (Logic Path)
El motor de extracciÃ³n (`src/loader.py`) opera bajo una lÃ³gica de **High Water Mark**:

1.  **State Detection:** El sistema inspecciona los metadatos de los archivos Parquet locales. La fecha mÃ¡xima detectada se convierte en el `start_date` para la consulta SQL.
2.  **Supabase Bridge:** Se utiliza un cliente `Realtime` y `Postgres` para realizar el fetch de los registros faltantes.
3.  **Merge & Persistence:**
    *   Si no hay datos locales, realiza una **Full Extraction**.
    *   Si hay datos locales, realiza un **Append** de los nuevos registros.
    *   **Formato de Salida:** Parquet (proporciona compresiÃ³n y preserva el esquema de tipos de datos de base).

---

## 3. ðŸ›¡ï¸ Protocolo de ValidaciÃ³n "Zero-Trust"
Para asegurar que el proyecto no sea contaminado con datos corruptos, cada extracciÃ³n ejecuta:

*   **Atomic Audit:** VerificaciÃ³n de tipos (Integer vs Float) y nombres de columnas contra el `config.yaml`.
*   **Integridad de Serie (Gap Check):** Algoritmo que detecta discontinuidades en las fechas. Si faltan dÃ­as, el reporte JSON emite una alerta crÃ­tica.
*   **Profiling CuantÃ­lico:** Se genera un perfil de 7 puntos (`min, p25, p50, p75, max, mean, std`) para detectar si los datos reciÃ©n llegados se desvÃ­an drÃ¡sticamente del comportamiento histÃ³rico (detecciÃ³n de deriva de datos).

---

## 4. âš™ï¸ ConfiguraciÃ³n y OrquestaciÃ³n (Reusable Blueprint)
Toda la lÃ³gica de extracciÃ³n es agnÃ³stica a la tabla gracias a la parametrizaciÃ³n en `config.yaml`:
*   **Rutas de Almacenamiento:** Centralizadas para evitar *hardcoding*.
*   **Esquemas Definidos:** El contrato de datos estÃ¡ escrito en YAML, permitiendo cambiar el origen de datos sin tocar el cÃ³digo Python.

---

## 5. ðŸ“‚ Artefactos y Trazabilidad (Protocolo Dual)
*   **Data Lake (Raw):** Archivos `.parquet` en `data/01_raw/`.
*   **BitÃ¡cora TÃ©cnica:** [phase_01_extractions_latest.json](file:///c:/Users/USUARIO/Documents/Forecaster/Tu_Bunuelito/outputs/reports/phase_01/phase_01_extractions_latest.json)

---
> [!NOTE]
> **Best Practice Reusable:** El uso de archivos Parquet permite que este loader sea compatible con herramientas de Big Data (Spark/Dask) en el futuro si el volumen de datos escala.
